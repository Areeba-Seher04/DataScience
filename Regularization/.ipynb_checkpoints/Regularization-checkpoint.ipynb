{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization is a very useful technique to improve our models and make sure that they don't overfit by controlling the model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model the model takes an error and minimize it. If we take a simple model its error will be greater than complex one as there is high possibility in complex model to overfit and it will not perform well on unseen data.\n",
    "\n",
    "let's understand it with an example of classification\n",
    "\n",
    "<img src=\"simple_vs_complex.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SIMPLE MODEL-LEFT ONE(LINEAR): \n",
    " - Two points are making mistakes here so error is larger than right one but it is much simpler\n",
    "2. COMPLEX MODEL-RIGHT ONE(POLYNOMIAL CURVE):\n",
    " - Error is small because zero mistakes but it's actually a bit more complicated\n",
    " - **Overfits** and dont generalize well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the equation of your SIMPLE model is \n",
    "$$\n",
    "3x_1 + 4x_2 + 5\n",
    "$$\n",
    "\n",
    "and equation of COMPLEX model is\n",
    "$$\n",
    "2x_1^3 - 2x_1^2x_2 - 4x_2^3 + 3x_1^2 + 6x_1x_2 + 4x_2^2 +5 \n",
    "$$\n",
    "\n",
    "SIMPLE MODEL has only 2 co-efficients(3 and 4) (ignore intercept) \n",
    "\n",
    "COMPLEX MODEL has 6 co-efficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we add that co-efficients in the error of our model?\n",
    "#### then complexity of the model will be added into the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE MODEL : sum of coefficients = 8\n",
    "\n",
    "Error after adding coefficients\n",
    "<img src=\"SIMPLE_ERROR.PNG\">\n",
    "COMPLEX MODEL: sum of coefficients = 21\n",
    "\n",
    "Error after adding coefficients\n",
    "<img src=\"COMPLEX_ERROR.PNG\">\n",
    "Now COMPLEX model have a large error than SIMPLE one.\n",
    "\n",
    "`\n",
    "You might think that complex problems require complex models, but many studies show that simpler models generally produce more precise predictions. Given several models with similar explanatory ability, the simplest is most likely to be the best choice. Start simple, and only make the model more complex as needed. The more complex you make your model, the more likely it is that you are tailoring the model to your dataset specifically, and generalizability suffers.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we cannot choose SIMPLE MODEL every time sometime we need COMPLEX MODEL but complex model overfits the data so here REGULARIZATION comes in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWO TYPES OF REGULARIZATION\n",
    "### 1. L1 REGULARIZATION\n",
    "It takes the co-efficients and just add the absoulate values of them to the error\n",
    "### 2. L1 REGULARIZATION\n",
    "It takes the co-effiecients and just add the squared values of them to the error\n",
    "\n",
    "`\n",
    "If we apply this on our example then in both cases COMPLEX MODEL gets PUNISHED MORE than SIMPLE MODEL.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THEN ON WHAT BASIS WE CAN CHOOSE COMPLEX MODEL?\n",
    "\n",
    "ANS: ON THE BASIS OF **LAMBDA PARAMETER**. FOR EVERY CASE WE HAVE TO DECIDE HOW MUCH WE WANT TO PUNISH COMPLEXITY IN EACH MODEL)\n",
    "\n",
    "Actually in REGULARIZATION we are not adding complexity part(coefficients) directly in ERROR. First we multiply it with LAMBDA parameter and then add it in ERROR.\n",
    "\n",
    "( LEFT ONE IS SHOWING SIMPLE MODEL, RIGHT ONE IS SHOWING COMPLEX MODEL)\n",
    "<img src=\"ERROR_LAMBDA.PNG\">\n",
    "##### SMALL LAMBDA\n",
    "if lambda is small then complex model wins as it has small combined error\n",
    "<img src=\"LAMBDA_SMALL.PNG\">\n",
    "##### LARGE LAMBDA\n",
    "if lambda is large then simple model wins\n",
    "<img src=\"LAMBDA_LARGE.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>LARGE LAMBDA</th>\n",
    "    <th>SMALL LAMBDA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>We are punishing the complexity by a large amount and we're picking simple model</td>\n",
    "    <td>We are punishing the complexity by a small amount and we're picking complex model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td>Models like recommendating potential friends on social media requires simplicity and should be faster to run on a big data. We are OK with some errors</td>\n",
    "    <td>Models related to medical requires low error and we're OK with some complexity.</td>\n",
    "    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANTS TO LEARN MORE ABOUT REGULARIZATION? (HOW REGULARIZATION  improve our models and make sure that they don't overfit)\n",
    "#### Why use regularization in polynomial regression instead of lowering the degree? src=https://stats.stackexchange.com/questions/226553/why-use-regularisation-in-polynomial-regression-instead-of-lowering-the-degree\n",
    "When doing regression, for example, two hyper parameters to choose are often the capacity of the function (eg. the largest exponent of a polynomial), and the amount of regularisation. What I'm confused about, is why not just choose a low capacity function, and then ignore any regularisation? In that way, it will not overfit. If I have a high capacity function together with regularisation, isn't that just the same as having a low capacity function and no regularisation? \n",
    "\n",
    "ANSWER: (Visit this link 1st answer is best)\n",
    "https://stats.stackexchange.com/questions/226553/why-use-regularisation-in-polynomial-regression-instead-of-lowering-the-degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
